{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "opening-lambda",
   "metadata": {},
   "source": [
    "### Pyspark Example\n",
    "data wrangling and training binary classifier to predict pairwise simialirity score for record linkage model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "animated-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import first\n",
    "from pyspark.sql.functions  import date_format\n",
    "from pyspark.sql.functions import lit,StringType\n",
    "\n",
    "from pyspark.sql.functions import row_number,udf,trim, upper, to_date, substring, length, min, when, format_number, dayofmonth, hour, dayofyear,  month, year, weekofyear, date_format, unix_timestamp\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import coalesce\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "import datetime\n",
    "from pyspark.sql.functions import year\n",
    "from pyspark.sql.functions import datediff,coalesce,lag\n",
    "from pyspark.sql.functions import when, to_date\n",
    "from pyspark.sql.functions import date_add\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "\n",
    "import traceback\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "reported-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "test='pairwise_similarity'\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "spark = SparkSession.builder \\\n",
    "            .appName(test) \\\n",
    "            .config('spark.sql.codegen.wholeStage', False) \\\n",
    "            .getOrCreate()\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "burning-orchestra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in data\n",
    "\n",
    "productnames_df=sqlContext.read.csv('./sample_30perc.csv',header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rocky-lounge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------------------------------------------+---+\n",
      "|source_id|product                                                  |id |\n",
      "+---------+---------------------------------------------------------+---+\n",
      "|5        |display Samsung UE-48H6270 LED Televizyon *              |11 |\n",
      "|1        |BOH SAMSUNG G850 GALAXY  final                           |42 |\n",
      "|9        |BOH Stdr1000201 25 1tb Bp Usb 3.0 GUMUS #                |33 |\n",
      "|13       |sample Samsung G950 Fiyatı #                             |7  |\n",
      "|18       | Samsung G150F Galaxy Alpha Cep Telefonu #               |84 |\n",
      "|1        | SAMSUNG G850 GALAXY  final                              |88 |\n",
      "|16       |$$ LG 42LB670V LED TV #                                  |63 |\n",
      "|6        |$$ Philips 185inç 193V5LSB2/62 5ms Led Monitör final     |31 |\n",
      "|17       | Samsung SM-T800 TABS 10.5 White Tablet                  |65 |\n",
      "|2        |BOH HP Pavilion 11-n000nt Pentium N3540 4GB  *           |44 |\n",
      "|6        |BOH Philips 185inç 193V5LSB2/62 5ms Led Monitör          |29 |\n",
      "|14       |sample Samsung Galaxy Alpha G650 Beyaz Cep Telefonu final|23 |\n",
      "|4        |NEW Samsung G750F Galaxy Alpha Cep Telefonu final        |90 |\n",
      "|9        | Stdr1000201 25 1tb Bp Usb 3.0 GUMUS final               |53 |\n",
      "|2        |BOH HP Pavilion 11-n000nt Pentium N3540 4GB              |13 |\n",
      "|6        |BOH Philips 185inç 193V5LSB2/62 5ms Led Monitör #        |47 |\n",
      "|6        |display Philips 185inç 193V5LSB2/62 5ms Led Monitör final|86 |\n",
      "|15       |display Tefal GV8930 Pro Express Buhar Kazanlı #         |32 |\n",
      "|8        |BOH 185 193v5lsb2-62 Led Monitor 5ms Siyah *             |27 |\n",
      "|14       |sample Samsung Galaxy Alpha G650 Beyaz Cep Telefonu      |73 |\n",
      "+---------+---------------------------------------------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "productnames_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "starting-compilation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create blocking_key\n",
    "\n",
    "source_id_df3=productnames_df.withColumn('blocking_key',lit('A'))\n",
    "\n",
    "\n",
    "\n",
    "# dev/testing\n",
    "# source_id_df3=source_id_df3.sample(0.0010).cache()\n",
    "\n",
    "def token_create(rwdf):\n",
    "\n",
    "    from pyspark.sql.functions import regexp_extract, split, coalesce\n",
    "\n",
    "    rwdf=rwdf.withColumn(\"product_num\", regexp_extract(\"product\", \"([0-9]+)\",1))\n",
    "\n",
    "\n",
    "    test_df=rwdf\n",
    "\n",
    "    \n",
    "    from pyspark.sql.functions import udf, col, lower, regexp_replace, array_remove\n",
    "    from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "    # removing punctuation and converting to lower case\n",
    "    test_df=test_df.alias('a').select(col('a.*'),\n",
    "                              regexp_replace(lower(trim(col('a.product'))), \"[^a-zA-Z\\\\s]\", \"\")\\\n",
    "                              .alias('input_product')\n",
    "                             )\n",
    "\n",
    "    # Tokenize text\n",
    "    tokenizer = Tokenizer(inputCol='input_product', outputCol='tokens')\n",
    "    df_words_token = tokenizer.transform(test_df)\n",
    "    \n",
    "\n",
    "    # Remove stop words\n",
    "    stop_word_list=['$$','new','sample', 'display','boh','final']\n",
    "\n",
    "    remover = StopWordsRemover(inputCol='tokens', outputCol='tokens_clean',stopWords=stop_word_list)\n",
    "    df_words_no_stopw = remover.transform(df_words_token).select(col('*'),\n",
    "                                                                 array_remove(col('tokens_clean'),'').alias('tokens_full')\n",
    "                                                                )\n",
    "\n",
    "    df_words_no_stopw = df_words_no_stopw[[x for x in df_words_no_stopw.columns if x not in ['input_product', 'tokens', 'tokens_clean']]]\n",
    "\n",
    "\n",
    "    df_words_no_stopw = df_words_no_stopw\\\n",
    "    .select(\"*\",*(coalesce(col('tokens_full').getItem(i),lit(\"\"))\\\n",
    "                            .alias('product{}'.format(i+1)) for i in range(2))).drop(\"tokens_full\")\n",
    "\n",
    "    return df_words_no_stopw\n",
    "\n",
    "# creating label features\n",
    "\n",
    "source_id_df4=token_create(source_id_df3)\n",
    "\n",
    "\n",
    "# cartesian join issue hack\n",
    "\n",
    "spark.conf.set(\"spark.sql.crossJoin.enabled\", True)\n",
    "\n",
    "\n",
    "\n",
    "# create pairs\n",
    "\n",
    "label_df=source_id_df4.alias('a')\\\n",
    ".join(\n",
    "source_id_df4.alias('b'),\n",
    "    col('a.blocking_key')==col('b.blocking_key'),\n",
    "    how='inner'\n",
    ")\\\n",
    ".select(\n",
    "col('a.product').alias('product_L'),\n",
    "    col('a.product_num').alias('product_num_L'),\n",
    "    col('a.product1').alias('product1_L'),\n",
    "    col('a.product2').alias('product2_L'),\n",
    "    col('a.source_id').alias('label_L'),\n",
    "    col('a.id').alias('id_L'),\n",
    "    col('b.product').alias('product_R'),\n",
    "    col('b.product_num').alias('product_num_R'),\n",
    "    col('b.product1').alias('product1_R'),\n",
    "    col('b.product2').alias('product2_R'),\n",
    "    col('b.source_id').alias('label_R'),\n",
    "    col('b.id').alias('id_R')\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "match_df=label_df.alias('a')\\\n",
    ".select(col('a.*'),\n",
    "       when(col('label_L')==col('label_R'),1).otherwise(0).alias('label_target')\n",
    "       )\n",
    "\n",
    "from pyspark.sql.functions import length,levenshtein\n",
    "\n",
    "#### ADD ANY NEW STRING FEATURES HERE TO CALCULATE EDIT DISTANCE ####\n",
    "\n",
    "col_list=['product',\n",
    "          'product1',\n",
    "          'product2']\n",
    "\n",
    "for x in range(len(col_list)):\n",
    "\n",
    "    if x==0:\n",
    "\n",
    "        match_df2=match_df.withColumn(col_list[x],coalesce(levenshtein(coalesce(col_list[x]+'_L',lit('N/A')),coalesce(col_list[x]+'_R',lit('N/A')))/length(col_list[x]+'_L'),lit(0.9999)))\n",
    "\n",
    "    else:\n",
    "\n",
    "        match_df2=match_df2.withColumn(col_list[x],coalesce(levenshtein(coalesce(col_list[x]+'_L',lit('N/A')),coalesce(col_list[x]+'_R',lit('N/A')))/length(col_list[x]+'_L'),lit(0.9999)))\n",
    "\n",
    "### ADD CATEGORICAL OR BINARY FEATURES HERE TO STRING INDEX ###\n",
    "\n",
    "match_df2=match_df2[[col_list+['product_num_L','product_num_R','label_target']+['id_L','id_R']]]\n",
    "\n",
    "\n",
    "# train binary classifier to predict pairwise matches\n",
    "# NOTE: calc fuzzy match scores between L and R features, \n",
    "# feed those as features to binary classifier model\n",
    "# can use levenshtein distance\n",
    "\n",
    "# string indexing some columns\n",
    "\n",
    "def add_string_index(df,index_cols):\n",
    "\n",
    "\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "    indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df) for column in index_cols ]\n",
    "\n",
    "\n",
    "    pipeline = Pipeline(stages=indexers)\n",
    "    df_r = pipeline.fit(df).transform(df)\n",
    "\n",
    "    df_n=df_r[[x for x in df_r.columns if x not in index_cols]]\n",
    "\n",
    "\n",
    "    for x in index_cols:\n",
    "\n",
    "        df_n=df_n.withColumnRenamed(x+'_index',x)\n",
    "\n",
    "    return df_n\n",
    "\n",
    "\n",
    "### ADD STRING OR CATEGORICAL FEATURES HERE TO STING INDEX ###\n",
    "\n",
    "match_df2=add_string_index(df=match_df2,index_cols=['product_num_L','product_num_R'])\n",
    "\n",
    "# Import VectorAssembler and Vectors\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "\n",
    "features=[x for x in match_df2.columns if x not in ['label_target','id_L','id_R']]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=features,outputCol=\"features\")\n",
    "\n",
    "output = assembler.transform(match_df2)\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier,DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# NOTE: keeping id_L, id_R to later join on to get similarity score result\n",
    "final_data = output.select('features','label_target','id_L','id_R')\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "sim_score = GBTClassifier(labelCol='label_target',\n",
    " featuresCol='features',\n",
    "  maxIter=10,\n",
    "  maxBins=3000,\n",
    "  maxDepth=6)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "earlier-specific",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "\n",
    "# set seed to keep training consistent\n",
    "sim_score.setSeed(123)\n",
    "\n",
    "sim_score_model = sim_score.fit(final_data)\n",
    "\n",
    "\n",
    "# save trained model for future use\n",
    "sim_score_model.write().overwrite().save('./'+test+'_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-chemistry",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
