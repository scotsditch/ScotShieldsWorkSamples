{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42c4827e",
   "metadata": {},
   "source": [
    "### Basic Agent\n",
    "- example of basic chat agent using a custom tool\n",
    "- custom tool scores pre-trained classifier on previously unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "647d1c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afb1cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8cda3b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat agent\n",
    "\n",
    "from config import api_key\n",
    "\n",
    "apikey=api_key\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = apikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4dee65ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "\n",
    "def llm_model_score(string):\n",
    "    \n",
    "    test_data_path,model_path=string.split(\",\")\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    # load test data\n",
    "    \n",
    "    test_data = pickle.load(open(test_data_path, 'rb'))\n",
    "    \n",
    "    X_test=test_data[[x for x in test_data.columns if x!='priceRange']]\n",
    "    \n",
    "    y_test=test_data['priceRange']\n",
    "    \n",
    "    # load model\n",
    "\n",
    "    loaded_model = pickle.load(open(model_path, 'rb'))\n",
    "    \n",
    "    result = loaded_model.score(X_test, y_test)\n",
    "    \n",
    "    return str(result)\n",
    "\n",
    "\n",
    "score_tool = Tool(\n",
    "    name='llm_model_score',\n",
    "    func= llm_model_score,\n",
    "    description=\"Useful for when you need to score a trained model on test data. The input to this tool should be a comma separated list of length 2 of strings representing the path to the saved test data and model.  For example 'saved_data.sav','saved_model.sav'. \"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "263e5c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.608\n"
     ]
    }
   ],
   "source": [
    "# test function\n",
    "print(llm_model_score(test_data_path='test_data.sav',model_path='best_model.sav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7643dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Set up the turbo LLM\n",
    "turbo_llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name='gpt-3.5-turbo'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fe29699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "\n",
    "\n",
    "tools = [score_tool]\n",
    "\n",
    "# conversational agent memory\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    k=3,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "\n",
    "# create our agent\n",
    "conversational_agent = initialize_agent(\n",
    "    agent='chat-conversational-react-description',\n",
    "    tools=tools,\n",
    "    llm=turbo_llm,\n",
    "#     llm=local_llm,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    early_stopping_method='generate',\n",
    "    memory=memory,\n",
    "    handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0b27a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"\"\"What is the score of the pretrained model saved at path best_model.sav on the new test data saved at path test_data.sav?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9ef06f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_react = f\"\"\"Question: What is the score of the pretrained model on the new test data?\n",
    "Thought: I need to score the pretrained model that is saved at model path best_model1.sav on the new test data saved at the path test_data1.sav.\n",
    "Action: score_tool['test_data1.sav','best_model1.sav']\n",
    "Observation: 0.75.\n",
    "Thought: The score returned was 0.75 so the model score on the test data is 0.75.\n",
    "Action: Finish[model score 0.75]\n",
    "\n",
    "Question:{question}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3385de6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2e7ba0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"llm_model_score\",\n",
      "    \"action_input\": \"test_data.sav,best_model.sav\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m0.608\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The score of the pretrained model saved at path best_model.sav on the new test data saved at path test_data.sav is 0.608.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"Question: What is the score of the pretrained model on the new test data?\\nThought: I need to score the pretrained model that is saved at model path best_model1.sav on the new test data saved at the path test_data1.sav.\\nAction: score_tool['test_data1.sav','best_model1.sav']\\nObservation: 0.75.\\nThought: The score returned was 0.75 so the model score on the test data is 0.75.\\nAction: Finish[model score 0.75]\\n\\nQuestion:What is the score of the pretrained model saved at path best_model.sav on the new test data saved at path test_data.sav?\",\n",
       " 'chat_history': [],\n",
       " 'output': 'The score of the pretrained model saved at path best_model.sav on the new test data saved at path test_data.sav is 0.608.'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_agent(manual_react)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8efd19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:LLMenv]",
   "language": "python",
   "name": "conda-env-LLMenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
