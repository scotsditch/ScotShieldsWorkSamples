{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "opening-lambda",
   "metadata": {},
   "source": [
    "### Record Linkage Example\n",
    "- deduping productnames in pyspark\n",
    "- datawrangling\n",
    "- loading pre-trained binary classifier\n",
    "- calculating pairwise similarity using binary classifier\n",
    "- clustering similarity scores with K-means to identify entities i.e. products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "animated-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import first\n",
    "from pyspark.sql.functions  import date_format\n",
    "from pyspark.sql.functions import lit,StringType\n",
    "\n",
    "from pyspark.sql.functions import row_number,udf,trim, upper, to_date, substring, length, min, when, format_number, dayofmonth, hour, dayofyear,  month, year, weekofyear, date_format, unix_timestamp\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import coalesce\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "import datetime\n",
    "from pyspark.sql.functions import year\n",
    "from pyspark.sql.functions import datediff,coalesce,lag\n",
    "from pyspark.sql.functions import when, to_date\n",
    "from pyspark.sql.functions import date_add\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "\n",
    "import traceback\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "reported-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "test='record_linkage'\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "spark = SparkSession.builder \\\n",
    "            .appName(test) \\\n",
    "            .config('spark.sql.codegen.wholeStage', False) \\\n",
    "            .getOrCreate()\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "burning-orchestra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in data\n",
    "\n",
    "productnames_df=sqlContext.read.csv('./sample_70perc.csv',header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rocky-lounge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------------------------------------------+---+\n",
      "|source_id|product                                                           |id |\n",
      "+---------+------------------------------------------------------------------+---+\n",
      "|13       | Samsung G950 Fiyatı                                              |1  |\n",
      "|10       | Dsc-w800 201mp 5x Optik 27 Lcd Dijital Kompakt Siyah             |3  |\n",
      "|5        |$$ Samsung UE-48H6270 LED Televizyon final                        |4  |\n",
      "|2        |NEW HP Pavilion 11-n000nt Pentium N3540 4GB                       |5  |\n",
      "|20       |display Sony cazoo Z2 Cep Telefonu                                |6  |\n",
      "|15       |NEW Tefal GV8930 Pro Express Buhar Kazanlı #                      |8  |\n",
      "|20       |BOH Sony cazoo Z2 Cep Telefonu final                              |9  |\n",
      "|17       |$$ Samsung SM-T800 TABS 105 White Tablet                          |10 |\n",
      "|12       | Sony Xperia Z2 Cep Telefonu *                                    |12 |\n",
      "|7        |NEW Sony Xperia M2 Dual ( Sony Türkiye Garantili ) final          |14 |\n",
      "|10       |display Dsc-w800 201mp 5x Optik 27 Lcd Dijital Kompakt Siyah final|16 |\n",
      "|16       |sample LG 42LB670V LED TV final                                   |17 |\n",
      "|1        |BOH SAMSUNG G850 GALAXY                                           |19 |\n",
      "|4        | Samsung G750F Galaxy Alpha Cep Telefonu *                        |20 |\n",
      "|16       |display LG 42LB670V LED TV *                                      |21 |\n",
      "|2        | HP Pavilion 11-n000nt Pentium N3540 4GB  *                       |22 |\n",
      "|12       |sample Sony Xperia Z2 Cep Telefonu #                              |24 |\n",
      "|3        | Apple iPhone 6 Plus 16GB Cep Telefonu #                          |25 |\n",
      "|15       |NEW Tefal GV8930 Pro Express Buhar Kazanlı *                      |26 |\n",
      "|15       |display Tefal GV8930 Pro Express Buhar Kazanlı *                  |30 |\n",
      "+---------+------------------------------------------------------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "productnames_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "starting-compilation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create blocking_key\n",
    "\n",
    "source_id_df3=productnames_df.withColumn('blocking_key',lit('A'))\n",
    "\n",
    "# string indexing some columns\n",
    "\n",
    "def add_string_index(df,index_cols):\n",
    "\n",
    "\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "    indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df) for column in index_cols ]\n",
    "\n",
    "\n",
    "    pipeline = Pipeline(stages=indexers)\n",
    "    df_r = pipeline.fit(df).transform(df)\n",
    "\n",
    "    df_n=df_r[[x for x in df_r.columns if x not in index_cols]]\n",
    "\n",
    "\n",
    "    for x in index_cols:\n",
    "\n",
    "        df_n=df_n.withColumnRenamed(x+'_index',x)\n",
    "\n",
    "    return df_n\n",
    "\n",
    "\n",
    "\n",
    "def token_create(rwdf):\n",
    "\n",
    "    from pyspark.sql.functions import regexp_extract, split, coalesce\n",
    "\n",
    "    rwdf=rwdf.withColumn(\"product_num\", regexp_extract(\"product\", \"([0-9]+)\",1))\n",
    "\n",
    "\n",
    "    test_df=rwdf\n",
    "\n",
    "    \n",
    "    from pyspark.sql.functions import udf, col, lower, regexp_replace, array_remove\n",
    "    from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "    # removing punctuation and converting to lower case\n",
    "    test_df=test_df.alias('a').select(col('a.*'),\n",
    "                              regexp_replace(lower(trim(col('a.product'))), \"[^a-zA-Z\\\\s]\", \"\")\\\n",
    "                              .alias('input_product')\n",
    "                             )\n",
    "\n",
    "    # Tokenize text\n",
    "    tokenizer = Tokenizer(inputCol='input_product', outputCol='tokens')\n",
    "    df_words_token = tokenizer.transform(test_df)\n",
    "    \n",
    "\n",
    "    # Remove stop words\n",
    "    stop_word_list=['$$','new','sample', 'display','boh','final']\n",
    "\n",
    "    remover = StopWordsRemover(inputCol='tokens', outputCol='tokens_clean',stopWords=stop_word_list)\n",
    "    df_words_no_stopw = remover.transform(df_words_token).select(col('*'),\n",
    "                                                                 array_remove(col('tokens_clean'),'').alias('tokens_full')\n",
    "                                                                )\n",
    "\n",
    "    df_words_no_stopw = df_words_no_stopw[[x for x in df_words_no_stopw.columns if x not in ['input_product', 'tokens', 'tokens_clean']]]\n",
    "\n",
    "\n",
    "    df_words_no_stopw = df_words_no_stopw\\\n",
    "    .select(\"*\",*(coalesce(col('tokens_full').getItem(i),lit(\"\"))\\\n",
    "                            .alias('product{}'.format(i+1)) for i in range(2))).drop(\"tokens_full\")\n",
    "\n",
    "    return df_words_no_stopw\n",
    "\n",
    "\n",
    "\n",
    "# for testing during development\n",
    "\n",
    "# source_id_df3=source_id_df3.sample(0.010)\n",
    "\n",
    "# source_id_df3.cache()\n",
    "\n",
    "\n",
    "dedupe_source=token_create(source_id_df3)\n",
    "\n",
    "### ADD NEW FEATURES HERE ####\n",
    "\n",
    "\n",
    "fldLst=[\n",
    "        'product', 'blocking_key','id', 'product_num', 'product1', 'product2']\n",
    "\n",
    "dedupe_source=dedupe_source[fldLst]\n",
    "\n",
    "### ADD NEW FEATURES SYMMETRICALLY HERE ###\n",
    "\n",
    "def create_dedupe_pairs(df):\n",
    "    \n",
    "    # cartesian join issue hack\n",
    "\n",
    "    spark.conf.set(\"spark.sql.crossJoin.enabled\", True)\n",
    "\n",
    "\n",
    "    label_df=df.alias('a')\\\n",
    "    .join(df.alias('b'),\n",
    "          (trim(col('a.blocking_key'))==trim(col('b.blocking_key'))),\n",
    "\n",
    "          how='inner'\n",
    "         )\\\n",
    "    .select(\n",
    "    col('a.product').alias('product_L'),\n",
    "    col('a.product_num').alias('product_num_L'),\n",
    "    col('a.product1').alias('product1_L'),\n",
    "    col('a.product2').alias('product2_L'),\n",
    "        col('a.id').alias('id_L'),\n",
    "    col('b.product').alias('product_R'),\n",
    "    col('b.product_num').alias('product_num_R'),\n",
    "    col('b.product1').alias('product1_R'),\n",
    "    col('b.product2').alias('product2_R'),\n",
    "        col('b.id').alias('id_R')\n",
    ")\n",
    "\n",
    "\n",
    "    match_df=label_df\n",
    "\n",
    "    # calculate similarity score between feature columns\n",
    "\n",
    "    ### ADD NEW STRING FEATURES HERE TO CALCULATE EDIT DISTANCE ###\n",
    "    \n",
    "    from pyspark.sql.functions import length,levenshtein\n",
    "\n",
    "    col_list=['product',\n",
    "          'product1',\n",
    "          'product2']\n",
    "\n",
    "    for x in range(len(col_list)):\n",
    "\n",
    "        if x==0:\n",
    "\n",
    "            match_df2=match_df.withColumn(col_list[x],coalesce(levenshtein(coalesce(col_list[x]+'_L',lit('N/A')),coalesce(col_list[x]+'_R',lit('N/A')))/length(col_list[x]+'_L'),lit(0.9999)))\n",
    "\n",
    "        else:\n",
    "\n",
    "            match_df2=match_df2.withColumn(col_list[x],coalesce(levenshtein(coalesce(col_list[x]+'_L',lit('N/A')),coalesce(col_list[x]+'_R',lit('N/A')))/length(col_list[x]+'_L'),lit(0.9999)))\n",
    "\n",
    "\n",
    "### ADD NEW BINARY OR CATEGORICAL FEATURES HERE ###\n",
    "\n",
    "    col_list+['blocking_key']\n",
    "\n",
    "    match_df2=match_df2[col_list+['product_num_L','product_num_R']+['id_L','id_R']]\n",
    "\n",
    "    return match_df2, match_df\n",
    "\n",
    "create_dedupe_pairs(df=dedupe_source)[1].count()\n",
    "\n",
    "# creating pairwise match probabilities\n",
    "\n",
    "product_pairs_df=create_dedupe_pairs(df=dedupe_source)[0]\n",
    "\n",
    "id_pairs_df=create_dedupe_pairs(df=dedupe_source)[1]\n",
    "\n",
    "\n",
    "### ADD NEW BINARY OR CATEGORICAL FEATURES HERE TO STRING INDEX ###\n",
    "\n",
    "\n",
    "product_pairs_df=add_string_index(df=product_pairs_df,index_cols=['product_num_L','product_num_R'])\n",
    "\n",
    "\n",
    "# calculate pairwise match probabilities\n",
    "\n",
    "### ADD NEW FEATURES TO LIST ####\n",
    "\n",
    "# Import VectorAssembler and Vectors\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# same columns model was trained with\n",
    "features=['product', 'product1', 'product2', 'product_num_L', 'product_num_R']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=features,outputCol=\"features\")\n",
    "\n",
    "output2 = assembler.transform(product_pairs_df)\n",
    "\n",
    "\n",
    "final_data2 = output2.select('features','id_L','id_R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "anonymous-music",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading pre-trained binary classifier to save time\n",
    "# binary classifier calcs pairwise similarity score\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier, GBTClassificationModel\n",
    "sim_score_model = GBTClassificationModel.load('./pairwise_similarity_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "earlier-specific",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = sim_score_model.transform(final_data2)\n",
    "\n",
    "# adding threshold to adjust recall/precision trade off\n",
    "# handy if classes are unbalanced (ex. matches very rare raise threshold)\n",
    "\n",
    "match_threshold=0.0\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# udf to extract match probability from probability vector in dataframe\n",
    "float_convert=udf(lambda v:float(v[1]),FloatType())\n",
    "\n",
    "\n",
    "predictions_df2=predictions_df.withColumn('match_prob',float_convert('probability')).alias('a')\\\n",
    ".select(col('a.*'),\n",
    "       when(col('match_prob')>match_threshold,col('match_prob')).otherwise(0.0).alias('adj_prob')\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "controlling-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating pivot table of productname similarity scores for clustering step\n",
    "\n",
    "# collect id values to speed up pivot step\n",
    "\n",
    "id_R_values=[x.id_R for x in predictions_df2.select('id_R').distinct().collect()]\n",
    "\n",
    "pivot_df = predictions_df2.groupBy('id_L').pivot('id_R',id_R_values).agg({'adj_prob':'max'})\n",
    "\n",
    "\n",
    "# fill null values with 0.0\n",
    "\n",
    "pivot_df2=pivot_df.na.fill(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ancient-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# estimated number of entities\n",
    "cluster_number=20\n",
    "\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feat_cols = [x for x in pivot_df2.columns if x!='id_L']\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols = feat_cols, outputCol='features')\n",
    "\n",
    "final_data = vec_assembler.transform(pivot_df2)\n",
    "\n",
    "kmeans3 = KMeans(featuresCol='features',k=cluster_number)\n",
    "\n",
    "model_k3 = kmeans3.fit(final_data)\n",
    "\n",
    "cluster_label_df=model_k3.transform(final_data)\n",
    "\n",
    "\n",
    "# adding cluster label to product attributes\n",
    "\n",
    "final_matched_df=dedupe_source.alias('a')\\\n",
    ".join(cluster_label_df.alias('b'),\n",
    "     (col('a.id')==col('b.id_L')),\n",
    "      how='inner'\n",
    "     )\\\n",
    ".select(\n",
    "col('a.*'),\n",
    "    col('b.prediction').alias('match_id')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "helpful-weather",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------+---+--------+\n",
      "|product                                                           |id |match_id|\n",
      "+------------------------------------------------------------------+---+--------+\n",
      "| SAPPHIRE 4GB R9 290X OC #                                        |37 |0       |\n",
      "| SAPPHIRE 4GB R9 290X OC                                          |58 |0       |\n",
      "|BOH SAPPHIRE 4GB R9 290X OC *                                     |39 |0       |\n",
      "|sample Dsc-w800 201mp 5x Optik 27 Lcd Dijital Kompakt Siyah final |36 |1       |\n",
      "|display Dsc-w800 201mp 5x Optik 27 Lcd Dijital Kompakt Siyah final|16 |1       |\n",
      "|$$ Dsc-w800 201mp 5x Optik 27 Lcd Dijital Kompakt Siyah #         |38 |1       |\n",
      "|sample Dsc-w800 201mp 5x Optik 27 Lcd Dijital Kompakt Siyah       |92 |1       |\n",
      "| Dsc-w800 201mp 5x Optik 27 Lcd Dijital Kompakt Siyah             |3  |1       |\n",
      "|BOH Dsc-w800 201mp 5x Optik 27 Lcd Dijital Kompakt Siyah final    |93 |1       |\n",
      "| Dsc-w800 201mp 5x Optik 27 Lcd Dijital Kompakt Siyah *           |40 |1       |\n",
      "|display Tefal GV8930 Pro Express Buhar Jeneratörlü Ütü #          |91 |2       |\n",
      "|sample Tefal GV8930 Pro Express Buhar Kazanlı final               |61 |2       |\n",
      "|display Tefal GV8930 Pro Express Buhar Jeneratörlü Ütü final      |75 |2       |\n",
      "|sample Tefal GV8930 Pro Express Buhar Jeneratörlü Ütü final       |96 |2       |\n",
      "|NEW Tefal GV8930 Pro Express Buhar Jeneratörlü Ütü *              |57 |2       |\n",
      "| Sony Xperia M2 Dual ( Sony Türkiye Garantili ) #                 |83 |3       |\n",
      "|NEW Sony Xperia M2 Dual ( Sony Türkiye Garantili ) final          |14 |3       |\n",
      "|NEW Sony Xperia M2 Dual ( Sony Türkiye Garantili )                |99 |3       |\n",
      "|display LG 42LB670V LED TV *                                      |21 |4       |\n",
      "|display LG 42LB670V LED TV final                                  |97 |4       |\n",
      "+------------------------------------------------------------------+---+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NOTE: match_id identifies deduped product\n",
    "\n",
    "final_matched_df[['product','id','match_id']]\\\n",
    ".orderBy(col('match_id')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-squad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
